{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/bitfromit2byte/Vision-Transformer/blob/main/Vision_Transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WBp8FnJTQQbF"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S8geKBs4QVqI"
   },
   "source": [
    "**Transform dataset & Create DataLoader**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uJTYOMmFQfcB"
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize(32),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.4914, 0.4822, 0.4465],\n",
    "                         std=[0.2470, 0.2435, 0.2616])\n",
    "])\n",
    "\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.CIFAR10(root='./data',train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IFcDmOAIDAx3",
    "outputId": "9a5890f1-61da-42db-9aaa-37fdedac72fb"
   },
   "outputs": [],
   "source": [
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KR1ZuH99Qz3_",
    "outputId": "81bbb886-5e46-4b7a-a29d-3ec739ce09df"
   },
   "outputs": [],
   "source": [
    "next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2o1OLNRqQ7CC"
   },
   "outputs": [],
   "source": [
    "# One of batch of images\n",
    "image_batch, label_batch = next(iter(train_loader))\n",
    "\n",
    "# map class indices to class names\n",
    "cifar10_classes = [\n",
    "    'airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "    'dog', 'frog', 'horse', 'ship','truck'\n",
    "]\n",
    "\n",
    "# get names for the batch\n",
    "label_names = [cifar10_classes[i] for i in label_batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "rCM4RGylQ9df",
    "outputId": "0465b58d-dc0a-4673-a892-aec8b40d357c"
   },
   "outputs": [],
   "source": [
    "# first image label\n",
    "label_names[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GvGF7jc-RBud",
    "outputId": "be6097af-151d-446e-d9de-560bf9039bd8"
   },
   "outputs": [],
   "source": [
    "image, label = image_batch[0], label_names[0]\n",
    "image.shape, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 465
    },
    "id": "c0sQw-53REAm",
    "outputId": "031e64cf-a230-4b91-e732-e6530078ca38"
   },
   "outputs": [],
   "source": [
    "plt.imshow(image.permute(1,2,0))\n",
    "plt.title(label)\n",
    "plt.axis(False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T3gqcr6PRF-t"
   },
   "source": [
    "**Reproducing the Vision Transformer Architecture**\n",
    "\n",
    "    based on paper 'An image is worth 16x16 words: transformers for image recognition at scale'\n",
    "    https://arxiv.org/abs/2010.11929\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "krlnEX4VRJ73",
    "outputId": "7033d1f0-89e4-4807-f485-dc3e9756ded0"
   },
   "outputs": [],
   "source": [
    "# Create example values\n",
    "height = 32\n",
    "width = 32\n",
    "color_channels = 3\n",
    "patch_size = 4\n",
    "\n",
    "# Number of patches\n",
    "number_of_patches = int((height * width) / patch_size**2)\n",
    "print(f'Number of patches (N) with image height (H={height}), width (W={width}) and patch size (P={patch_size}): {number_of_patches}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zzaL_uzfRSwZ",
    "outputId": "c79837b0-87fd-458e-8914-372ecdead534"
   },
   "outputs": [],
   "source": [
    "# Input shape (size of single image)\n",
    "embedding_input = (height, width, color_channels)\n",
    "\n",
    "# Output shape\n",
    "embedding_output = (number_of_patches, patch_size**2 * color_channels)\n",
    "\n",
    "print(f'Input shape (single 2d image): {embedding_input}')\n",
    "print(f'Output shape (flattened 2d image into patches): {embedding_output}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 752
    },
    "id": "5tBAixYARZ16",
    "outputId": "b5c5bea4-7584-4fbf-a77c-769a4552c8a1"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "image_permuted = image.permute(1, 2, 0)\n",
    "image_size = 32\n",
    "patch_size = 4\n",
    "num_patches = image_size // patch_size\n",
    "\n",
    "fig, axs = plt.subplots(\n",
    "    nrows=num_patches,\n",
    "    ncols=num_patches,\n",
    "    figsize=(num_patches, num_patches),\n",
    "    sharex=True,\n",
    "    sharey=True\n",
    ")\n",
    "\n",
    "for i, patch_height in enumerate(range(0, image_size, patch_size)):\n",
    "    for j, patch_width in enumerate(range(0, image_size, patch_size)):\n",
    "\n",
    "        patch = image_permuted[\n",
    "            patch_height:patch_height+patch_size,\n",
    "            patch_width:patch_width+patch_size,\n",
    "            :\n",
    "        ]\n",
    "\n",
    "        axs[i, j].imshow(np.clip(patch, 0, 1))\n",
    "        axs[i, j].set_ylabel(i + 1,\n",
    "                             rotation='horizontal',\n",
    "                             horizontalalignment='right',\n",
    "                             verticalalignment='center')\n",
    "        axs[i, j].set_xlabel(j + 1)\n",
    "        axs[i, j].set_xticks([])\n",
    "        axs[i, j].set_yticks([])\n",
    "        axs[i, j].label_outer()\n",
    "\n",
    "fig.suptitle(f'{label_names[0]}', fontsize=16)\n",
    "plt.show()\n",
    "plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4rozA5EYRcV7"
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "patch_size = 4\n",
    "\n",
    "conv2d = nn.Conv2d(in_channels=3,\n",
    "                   out_channels=128,\n",
    "                   kernel_size=patch_size,\n",
    "                   stride=patch_size,\n",
    "                   padding=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QuogUWkRV8lb",
    "outputId": "6a0b6fb0-6032-4975-a18b-8670e4146f2a"
   },
   "outputs": [],
   "source": [
    "convolution_output = conv2d(image.unsqueeze(0))\n",
    "print(convolution_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 214
    },
    "id": "dBEs5vCRV9ih",
    "outputId": "aea328cc-9d60-4c49-fbd6-0cf65e562864"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "random_indexes = random.sample(range(0,128), k=5)\n",
    "print(f'Showing random convolutional feature maps from indexes: {random_indexes}')\n",
    "\n",
    "# Create plot\n",
    "fig, axs = plt.subplots(nrows=1, ncols=5, figsize=(12,12))\n",
    "\n",
    "for i, idx in enumerate(random_indexes):\n",
    "    convolution_feature_map = convolution_output[:, idx, :, :] # index on the output tensor of the convolutional layer\n",
    "    axs[i].imshow(convolution_feature_map.squeeze().detach().numpy())\n",
    "    axs[i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8jP-PSucWHJx"
   },
   "outputs": [],
   "source": [
    "flatten = nn.Flatten(start_dim=2,\n",
    "                     end_dim=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_GnhbEQJWNFy"
   },
   "outputs": [],
   "source": [
    "# Flatten output\n",
    "flattened_image = flatten(convolution_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2Dcvn-e2WNfm"
   },
   "outputs": [],
   "source": [
    "# batch_size, num_patches, embedding_size\n",
    "batch_patches_emb = flattened_image.permute(0,2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "id": "a1dTHJtHWTJH",
    "outputId": "ddf9ad83-a038-486b-e79d-05c18815e34f"
   },
   "outputs": [],
   "source": [
    "flattened_feature_map = batch_patches_emb[:, :, 0]\n",
    "\n",
    "# Plot flattened feature map visually\n",
    "plt.figure(figsize=(22,22))\n",
    "plt.imshow(flattened_feature_map.detach().numpy())\n",
    "plt.title(f'Flattened feature map shape: {flattened_feature_map.shape}')\n",
    "plt.axis(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NM2d5vUGWTjt"
   },
   "source": [
    "**Patch Embedding Module**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aP_rejATWvb0"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class Embedding(nn.Module):\n",
    "    '''Turns 2d input image into a learnable embedding'''\n",
    "    def __init__(self,\n",
    "                 in_channels:int=3,\n",
    "                 patch_size:int=4,\n",
    "                 embedding_dim:int=128):\n",
    "        super().__init__()\n",
    "\n",
    "        self.patcher = nn.Conv2d(in_channels=in_channels,\n",
    "                                 out_channels=embedding_dim,\n",
    "                                 kernel_size=patch_size,\n",
    "                                 stride=patch_size,\n",
    "                                 padding=0)\n",
    "\n",
    "        self.flatten = nn.Flatten(start_dim=2,\n",
    "                                  end_dim=3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        image_resolution = x.shape[-1]\n",
    "\n",
    "        x_patch = self.patcher(x)\n",
    "        x_flattened = self.flatten(x_patch)\n",
    "        return x_flattened.permute(0,2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nRVxFa6-Wxz6"
   },
   "outputs": [],
   "source": [
    "class MultiheadSelfAttention(nn.Module):\n",
    "    def __init__(self,\n",
    "                 embedding_dim:int=128,\n",
    "                 num_heads:int=4,\n",
    "                 attn_dropout:float=0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer_norm = nn.LayerNorm(normalized_shape=embedding_dim)\n",
    "\n",
    "        self.multihead_attn = nn.MultiheadAttention(embed_dim=embedding_dim,\n",
    "                                                    num_heads=num_heads,\n",
    "                                                    dropout=attn_dropout,\n",
    "                                                    batch_first=True)\n",
    "    def forward(self, x):\n",
    "        x = self.layer_norm(x)\n",
    "        attn_output, _ = self.multihead_attn(query=x,\n",
    "                                             key=x,\n",
    "                                             value=x,\n",
    "                                             need_weights=False)\n",
    "        return attn_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jlu6P5umW_ih"
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    '''Creates a layer normalized multilayer perceptron block'''\n",
    "    def __init__(self,\n",
    "                 embedding_dim:int=128,\n",
    "                 mlp_size:int=256,\n",
    "                 dropout:float=0.1):\n",
    "        super().__init__()\n",
    "\n",
    "        # Normalization layer (LN)\n",
    "        self.layer_norm = nn.LayerNorm(normalized_shape=embedding_dim)\n",
    "\n",
    "        # MLP layer\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(in_features=embedding_dim,\n",
    "                      out_features=mlp_size),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=mlp_size,\n",
    "                      out_features=embedding_dim),\n",
    "            nn.Dropout(p=dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer_norm(x)\n",
    "        x = self.mlp(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zohOWR5YXFlS"
   },
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 embedding_dim:int=128,\n",
    "                 num_heads:int=4,\n",
    "                 mlp_size:int=256,\n",
    "                 mlp_dropout:float=0.1,\n",
    "                 attn_dropout:float=0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.msa = MultiheadSelfAttention(embedding_dim=embedding_dim,\n",
    "                                          num_heads=num_heads,\n",
    "                                          attn_dropout=attn_dropout)\n",
    "\n",
    "        self.mlp = MLP(embedding_dim=embedding_dim,\n",
    "                       mlp_size=mlp_size,\n",
    "                       dropout=mlp_dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.msa(x) + x\n",
    "\n",
    "        x = self.mlp(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9IB-ZHCvXHj-"
   },
   "outputs": [],
   "source": [
    "transformer_encoder = TransformerEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jtvfo05SXJv-"
   },
   "outputs": [],
   "source": [
    "torch_transformer_encoder_layer = nn.TransformerEncoderLayer(d_model=128,\n",
    "                                                             nhead=4,\n",
    "                                                             dim_feedforward=256,\n",
    "                                                             dropout=0.1,\n",
    "                                                             activation='gelu',\n",
    "                                                             batch_first=True,\n",
    "                                                             norm_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tny6yvpxXSnR"
   },
   "source": [
    "**Vision Transformer Architecture**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yDGaSc_pXaCK"
   },
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 image_size=32,\n",
    "                 channels=3,\n",
    "                 patch_size=4,\n",
    "                 transformer_layers=6,\n",
    "                 embedding_dim=128,\n",
    "                 mlp_size=256,\n",
    "                 num_heads=4,\n",
    "                 attn_dropout=0,\n",
    "                 mlp_dropout=0.1,\n",
    "                 embedding_dropout=0.1,\n",
    "                 num_classes=10):\n",
    "        super().__init__()\n",
    "\n",
    "        # number of patches\n",
    "        self.patch_count = (image_size**2) // patch_size**2\n",
    "\n",
    "        # learnable class embeddings\n",
    "        self.class_emb = nn.Parameter(data=torch.randn(1, 1, embedding_dim),\n",
    "                                     requires_grad=True)\n",
    "\n",
    "        # learnable position embedding\n",
    "        self.position_emb = nn.Parameter(data=torch.randn(1, self.patch_count+1, embedding_dim),\n",
    "                                             requires_grad=True)\n",
    "\n",
    "        # embedding dropout\n",
    "        self.emb_dropout = nn.Dropout(p=embedding_dropout)\n",
    "\n",
    "        # patch embedding layer\n",
    "        self.patch_emb = Embedding(in_channels=channels,\n",
    "                                   patch_size=patch_size,\n",
    "                                   embedding_dim=embedding_dim)\n",
    "\n",
    "        # Transformer Encoder blocks\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embedding_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=mlp_size,\n",
    "            dropout=mlp_dropout,\n",
    "            activation='gelu',\n",
    "            batch_first=True,\n",
    "            norm_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            encoder_layer,\n",
    "            num_layers=transformer_layers\n",
    "        )\n",
    "\n",
    "        # Classifier Head\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.LayerNorm(normalized_shape=embedding_dim),\n",
    "            nn.Linear(in_features=embedding_dim,\n",
    "                      out_features=num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        batch_size = x.shape[0]\n",
    "\n",
    "        class_token = self.class_emb.expand(batch_size, -1, -1)\n",
    "\n",
    "        x = self.patch_emb(x)\n",
    "\n",
    "        x = torch.cat((class_token, x), dim=1)\n",
    "\n",
    "        x = self.position_emb + x\n",
    "\n",
    "        x = self.emb_dropout(x)\n",
    "\n",
    "        x = self.transformer_encoder(x)\n",
    "\n",
    "        x = self.classifier(x[:, 0])\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cJlq9mcVXgWF"
   },
   "source": [
    "I chose to use PyTorch's transformer encoder instead since it's less error-prone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UNRgy5fYXobU",
    "outputId": "a39cc650-0047-4682-bb5b-def10ae7f7c5"
   },
   "outputs": [],
   "source": [
    "!pip install torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Mxage8kHXhP7",
    "outputId": "88c52280-0726-4455-d56c-2626c58228df"
   },
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "\n",
    "model = ViT()\n",
    "summary(model=model,\n",
    "        input_size=(32, 3, 32, 32), # (batch_size, color_channels, height, width)\n",
    "        # col_names=[\"input_size\"], # uncomment for smaller output\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "        row_settings=[\"var_names\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ejg7sjmyXxaw"
   },
   "source": [
    "**Training the Vision Transformer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k0Dtft3ufXa2"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def train_epoch(model: torch.nn.Module,\n",
    "                dataloader: torch.utils.data.DataLoader,\n",
    "                loss_fn: torch.nn.Module,\n",
    "                optimizer: torch.optim.Optimizer,\n",
    "                device: torch.device):\n",
    "    model.train()\n",
    "    train_loss, train_acc = 0, 0\n",
    "\n",
    "    for batch, (input, target) in enumerate(dataloader):\n",
    "        input, target = input.to(device), target.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        y_pred = model(input)\n",
    "\n",
    "        # Calculate and accumulate loss\n",
    "        loss = loss_fn(y_pred, target)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # Optimizer zero grad\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Loss backward\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        # Calculate cumulative accuracy\n",
    "        predicted_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n",
    "        train_acc += (predicted_class == target).sum().item()/len(y_pred)\n",
    "\n",
    "    train_loss = train_loss / len(dataloader)\n",
    "    train_acc = train_acc / len(dataloader)\n",
    "    return { 'loss': train_loss,\n",
    "             'accuracy': train_acc\n",
    "           }\n",
    "\n",
    "def evaluate(model: torch.nn.Module,\n",
    "             dataloader: torch.utils.data.DataLoader,\n",
    "             loss_fn: torch.nn.Module,\n",
    "             device: torch.device):\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    test_loss, test_acc = 0, 0\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for batch, (inputs, targets) in enumerate(dataloader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            # calculate cumulative accuracy\n",
    "            predicted_test_labels = outputs.argmax(dim=1)\n",
    "            test_acc += ((predicted_test_labels == targets).sum().item()/len(predicted_test_labels))\n",
    "\n",
    "    test_loss = test_loss / len(dataloader)\n",
    "    test_acc = test_acc / len(dataloader)\n",
    "    return {\n",
    "        \"loss\": test_loss,\n",
    "        \"accuracy\": test_acc\n",
    "    }\n",
    "\n",
    "\n",
    "def train(model: torch.nn.Module,\n",
    "          train_dataloader: torch.utils.data.DataLoader,\n",
    "          test_dataloader: torch.utils.data.DataLoader,\n",
    "          optimizer: torch.optim.Optimizer,\n",
    "          loss_fn: torch.nn.Module,\n",
    "          epochs: int,\n",
    "          device: torch.device):\n",
    "\n",
    "    history = {'train_loss': [], 'train_acc': [],\n",
    "               'test_loss': [], 'test_acc': []}\n",
    "\n",
    "    for epoch in tqdm(range(epochs)):\n",
    "\n",
    "        train_metrics = train_epoch(model, train_dataloader, loss_fn, optimizer, device)\n",
    "        test_metrics = evaluate(model, test_dataloader, loss_fn, device)\n",
    "\n",
    "        history['train_loss'].append(train_metrics['loss'])\n",
    "        history['train_acc'].append(train_metrics['accuracy'])\n",
    "        history['test_loss'].append(test_metrics['loss'])\n",
    "        history['test_acc'].append(test_metrics['accuracy'])\n",
    "\n",
    "        print(f'Epoch {epoch}/{epochs}')\n",
    "        print(f\"Train Loss: {train_metrics['loss']:.4f} | \"\n",
    "              f\"Train Acc: {train_metrics['accuracy'] * 100:.2f}%\")\n",
    "        print(f\"Test Loss: {test_metrics['loss']:.4f} | \"\n",
    "              f\"Test Acc: {test_metrics['accuracy'] * 100:.2f}%\")\n",
    "        print('-' * 50)\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 605,
     "referenced_widgets": [
      "18e180fa299d4811a6b195d27d4058b8",
      "ff5cfd39afbd4b6f9aee021d4fe2c0bd",
      "b99bd7f190d84934aa76eef6f34dba95",
      "b9687bcade714d208aae98363ad90bf2",
      "853052cf505d47358bfe423c4a1a4a0e",
      "2723cd03d7e34d1a9974369fe672a6f5",
      "9b1228e1a9824acf9f69785a5d535ee6",
      "ca24c7451ba5480986c6fe9e6cfa9444",
      "659b0cdcfa8f4d40a1ac18e5a3ae989c",
      "47eebd999e1b4aae9934809fcb8e7dd4",
      "561b21b883e64e6d96b57a7edcce6c68"
     ]
    },
    "id": "FbTEJMf4Xvth",
    "outputId": "589ea4ba-75b3-40cc-9d66-63cdccb0675e"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# seed for consistent output\n",
    "torch.manual_seed(42)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "#Create optimizer\n",
    "optimizer = torch.optim.Adam(params=model.parameters(),\n",
    "                             lr=3e-3,\n",
    "                             betas=(0.9, 0.999),\n",
    "                             weight_decay=0.3)\n",
    "\n",
    "# Loss function for multi-output classfication\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "results = train(model=model,\n",
    "                train_dataloader=train_loader,\n",
    "                test_dataloader=test_loader,\n",
    "                optimizer=optimizer,\n",
    "                loss_fn=loss_fn,\n",
    "                epochs=8,\n",
    "                device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S0pp7RtReU3-"
   },
   "outputs": [],
   "source": [
    "def plot_loss_curves(results):\n",
    "\n",
    "    # Get the loss values of the results dictionary (training and test)\n",
    "    loss = results['train_loss']\n",
    "    test_loss = results['test_loss']\n",
    "\n",
    "    # Get the accuracy values of the results dictionary (training and test)\n",
    "    accuracy = results['train_acc']\n",
    "    test_accuracy = results['test_acc']\n",
    "\n",
    "    # Figure out how many epochs there were\n",
    "    epochs = range(len(results['train_loss']))\n",
    "\n",
    "    # Setup a plot\n",
    "    plt.figure(figsize=(15, 7))\n",
    "\n",
    "    # Plot loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, loss, label='train_loss')\n",
    "    plt.plot(epochs, test_loss, label='test_loss')\n",
    "    plt.title('Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, accuracy, label='train_accuracy')\n",
    "    plt.plot(epochs, test_accuracy, label='test_accuracy')\n",
    "    plt.title('Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 525
    },
    "id": "IoHxogxLIufB",
    "outputId": "dfd39763-b636-4a9e-d954-4effb25624ce"
   },
   "outputs": [],
   "source": [
    "plot_loss_curves(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-4CaeasNI0yF"
   },
   "source": [
    "**Pretrained model of ViT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jXNiYNXGJYKQ",
    "outputId": "dcb27e41-3325-4b7a-869f-d19974b2cbe7"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "print(torch.__version__)\n",
    "print(torchvision.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "zzkYPu51Je-N",
    "outputId": "dad50de4-299b-4ae6-8b8a-bc59d93ea533"
   },
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5UN4UbrrJira"
   },
   "outputs": [],
   "source": [
    "pretrained_vit_weights = torchvision.models.ViT_B_16_Weights.DEFAULT # requires\n",
    "\n",
    "# 2. Setup a ViT model instance with pretrained weights\n",
    "weights = torchvision.models.ViT_B_16_Weights.DEFAULT\n",
    "pretrained_vit = torchvision.models.vit_b_16(weights=weights).to(device)\n",
    "\n",
    "\n",
    "# 3. Freeze the base parameters\n",
    "for parameter in pretrained_vit.parameters():\n",
    "    parameter.requires_grad = False\n",
    "\n",
    "# 4. Change the classifier head (set the seeds to ensure same initialization with linear head)\n",
    "torch.manual_seed(42)\n",
    "pretrained_vit.heads = nn.Linear(\n",
    "    in_features=768,  # usually 768\n",
    "    out_features=10                                # CIFAR-10 classes\n",
    ").to(device)\n",
    "# pretrained_vit # uncomment for model output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jqjIh5NOKwcd",
    "outputId": "93f0cb3e-05f3-4a2c-bba8-1b8bb1b53985"
   },
   "outputs": [],
   "source": [
    "pretrained_vit_transforms = pretrained_vit_weights.transforms()\n",
    "print(pretrained_vit_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fbE5N_llLC8I"
   },
   "outputs": [],
   "source": [
    "\n",
    "train_dataset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.CIFAR10(root='./data',train=False, download=True, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 605,
     "referenced_widgets": [
      "1487a3bedeaa456aab68c990318ad895",
      "8a2c1d9b99394e68b4ffc22a2654b98c",
      "a91075facad7464f8c1b440e42a762f3",
      "91ece9010272401ab95462684d2a5991",
      "8f3b1aa9d3bd4f3b8b6986bb44ff171f",
      "15e77733ab384d5184622b1bb448df75",
      "ae2da3f407f84f8b8ad52398516593f8",
      "19b7c5fe8aa8493691c60ab86d0675d8",
      "f4c9362bccaf4811ae9a92c1b5e2da0c",
      "9035c2a6c8384780a0561aa31e63d8a0",
      "20bfdb5857484cbaa32616767cf101a0"
     ]
    },
    "id": "G1Ubo-nFLg-R",
    "outputId": "e45fbaa7-bca6-4913-ea01-bb653c3ebf9a"
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(params=pretrained_vit.heads.parameters(),\n",
    "                             lr=1e-3)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "torch.manual_seed(42)\n",
    "pretrained_vit_results = train(model=pretrained_vit,\n",
    "                train_dataloader=train_loader,\n",
    "                test_dataloader=test_loader,\n",
    "                optimizer=optimizer,\n",
    "                loss_fn=loss_fn,\n",
    "                epochs=8,\n",
    "                device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 536
    },
    "id": "nZjgIrn3MVuO",
    "outputId": "f249a0e5-4927-486c-df74-3367044f2565"
   },
   "outputs": [],
   "source": [
    "plot_loss_curves(pretrained_vit_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dyMhPe6TAu9e"
   },
   "source": [
    "**Prediction made by Pretrained ViT on CIFAR-10 image of a frog**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YgOGyDCbySrM",
    "outputId": "99cc2628-7c5a-4b9a-c672-0f1d6b5e831c"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "target_dir = 'models'\n",
    "\n",
    "target_dir_path = Path(target_dir)\n",
    "\n",
    "target_dir_path.mkdir(parents=True,\n",
    "                      exist_ok=True)\n",
    "\n",
    "model_name='pretrained_vit_feature_extractor'\n",
    "\n",
    "model_save_path = target_dir_path / model_name\n",
    "\n",
    "print(f\"Model saved in: {model_save_path}\")\n",
    "torch.save(obj=pretrained_vit.state_dict(),\n",
    "           f=model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wyTh71h_59Fs",
    "outputId": "fad7b0a5-1f19-47e6-fe55-38903e31fc6d"
   },
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "\n",
    "# Load CIFAR-10 without transforms (important!)\n",
    "dataset = datasets.CIFAR10(root=\"./data\", train=True, download=True)\n",
    "\n",
    "# Pick an image (change index if you want)\n",
    "image, label = dataset[0]\n",
    "\n",
    "# Save it\n",
    "image.save(\"cifar10_sample.png\")\n",
    "\n",
    "print(\"Saved cifar10_sample.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "I0rbDxNJ_QKU",
    "outputId": "458d58e3-f2ee-41e9-e41d-048cccd004da"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "files.download(\"cifar10_sample.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "id": "4PGZwdd1_Vil",
    "outputId": "16f230a4-9931-44df-8839-a990ff802f9a"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AXU3eplw_3mB"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "custom_image_path = next(iter(uploaded))\n",
    "image = Image.open(custom_image_path).convert('RGB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 428
    },
    "id": "1GnKCyVI7-0O",
    "outputId": "b255c0f4-aa58-4aa8-8fa3-487f6b314e47"
   },
   "outputs": [],
   "source": [
    "image_transform = transforms.Compose([\n",
    "            transforms.Resize(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "\n",
    "pretrained_vit.to(device)\n",
    "\n",
    "pretrained_vit.eval()\n",
    "with torch.inference_mode():\n",
    "  transformed_image = image_transform(image).unsqueeze(dim=0)\n",
    "\n",
    "  target_image_pred = pretrained_vit(transformed_image.to(device))\n",
    "\n",
    "  target_image_pred_probs = torch.softmax(target_image_pred, dim=1)\n",
    "\n",
    "  target_image_pred_label = torch.argmax(target_image_pred_probs, dim=1)\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(image)\n",
    "plt.title(f\"Pred: {label_names[target_image_pred_label]} | Prob: {target_image_pred_probs.max():.3f}\")\n",
    "plt.axis(False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WE6zGZK698Hp"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/bitfromit2byte/Vision-Transformer.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wVv0fQLpD248"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPtuHgG8wAVDVacD4SKKQsH",
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
